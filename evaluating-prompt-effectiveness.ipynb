{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c461dc23-bfaa-4ec6-9175-06ca27f87d5f",
   "metadata": {},
   "source": [
    "### Evaluating Prompt Effectiveness\n",
    "#### Overview\n",
    "This tutorial focuses on methods and techniques for evaluating the effectiveness of prompts in AI language models. We'll explore various metrics for measuring prompt performance and discuss both manual and automated evaluation techniques.\n",
    "\n",
    "#### Motivation\n",
    "As prompt engineering becomes increasingly crucial in AI applications, it's essential to have robust methods for assessing prompt effectiveness. This enables developers and researchers to optimize their prompts, leading to better AI model performance and more reliable outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "528922fb-27af-4f05-9bc8-5b668495abd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0820 20:23:56.680000 14112 torch\\distributed\\elastic\\multiprocessing\\redirects.py:29] NOTE: Redirects are currently not supported in Windows or MacOs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Revathi\\AppData\\Roaming\\Python\\Python312\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "\n",
    "# Load environment variables\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# Set up OpenAI API key\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "# Initialize the language model\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "# Initialize sentence transformer for semantic similarity\n",
    "sentence_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "def semantic_similarity(text1, text2):\n",
    "    \"\"\"Calculate semantic similarity between two texts using cosine similarity.\"\"\"\n",
    "    embeddings = sentence_model.encode([text1, text2])\n",
    "    return cosine_similarity([embeddings[0]], [embeddings[1]])[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe5a8b2-68f0-42ea-961f-8ba81c30f140",
   "metadata": {},
   "source": [
    "#### Metrics for Measuring Prompt Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ab52299c-5446-41c7-83db-078ed6c75777",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relevance_score(response, expected_content):\n",
    "    \"\"\"Calculate relevance score based on semantic similarity to expected content.\"\"\"\n",
    "    return semantic_similarity(response, expected_content)\n",
    "\n",
    "def consistency_score(responses):\n",
    "    \"\"\"Calculate consistency score based on similarity between multiple responses.\"\"\"\n",
    "    if len(responses) < 2:\n",
    "        return 1.0  # Perfect consistency if there's only one response\n",
    "    similarities = []\n",
    "    for i in range(len(responses)):\n",
    "        for j in range(i+1, len(responses)):\n",
    "            similarities.append(semantic_similarity(responses[i], responses[j]))\n",
    "    return np.mean(similarities)\n",
    "\n",
    "def specificity_score(response):\n",
    "    \"\"\"Calculate specificity score based on response length and unique word count.\"\"\"\n",
    "    words = response.split()\n",
    "    unique_words = set(words)\n",
    "    return len(unique_words) / len(words) if words else 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "804e791a-3543-4b0c-bf10-33df6aef4cd4",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b154cc18-4aaa-49d5-a23a-33d06d5f6876",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: Explain the concept of machine learning in simple terms.\n",
      "Response: Machine learning is a type of technology that allows computers to learn from data and improve their performance on a task without being explicitly programmed for it. \n",
      "\n",
      "Here’s how it works in simple terms:\n",
      "\n",
      "1. **Data**: First, we gather a lot of information (data) about a specific task. For example, if we want to teach a computer to recognize cats in photos, we would collect many pictures of cats and also pictures of other things.\n",
      "\n",
      "2. **Learning**: Instead of writing specific rules for the computer about how to identify a cat, we use algorithms—these are like recipes—that analyze the data. The computer looks for patterns and features that help distinguish cats from non-cats.\n",
      "\n",
      "3. **Training**: The computer uses a part of this data to \"train\" itself. It makes guesses about which images contain cats and checks its answers against the correct outcomes (whether each image actually has a cat or not). The more guesses it makes, the better it gets, as it learns from its mistakes.\n",
      "\n",
      "4. **Testing**: After training, we test the computer with new images it hasn’t seen before to see how well it learned. If it successfully identifies many of them correctly, it means it has learned to recognize cats.\n",
      "\n",
      "5. **Improvement**: As we give the machine more data, it can continue to learn and improve over time, getting more accurate at its tasks. \n",
      "\n",
      "So, machine learning is like teaching a computer to learn from experience, just like humans do, but using lots of data and mathematical models instead of explicit instructions.\n",
      "\n",
      "Evaluation Criteria:\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Score for Clarity (0-10):  9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clarity: 9.0/10\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Score for Accuracy (0-10):  8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 8.0/10\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Score for Simplicity (0-10):  10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simplicity: 10.0/10\n",
      "\n",
      "Additional Comments:\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter any additional comments:  best prompt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comments: best prompt\n"
     ]
    }
   ],
   "source": [
    "def manual_evaluation(prompt, response, criteria):\n",
    "    \"\"\"Simulate manual evaluation of a prompt-response pair.\"\"\"\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    print(f\"Response: {response}\")\n",
    "    print(\"\\nEvaluation Criteria:\")\n",
    "    for criterion in criteria:\n",
    "        score = float(input(f\"Score for {criterion} (0-10): \"))\n",
    "        print(f\"{criterion}: {score}/10\")\n",
    "    print(\"\\nAdditional Comments:\")\n",
    "    comments = input(\"Enter any additional comments: \")\n",
    "    print(f\"Comments: {comments}\")\n",
    "\n",
    "# Example usage\n",
    "prompt = \"Explain the concept of machine learning in simple terms.\"\n",
    "response = llm.invoke(prompt).content\n",
    "criteria = [\"Clarity\", \"Accuracy\", \"Simplicity\"]\n",
    "manual_evaluation(prompt, response, criteria)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "23a3b244-251c-4606-865a-6e432cfc1a2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Revathi\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1784: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: What are the three main types of machine learning?\n",
      "Response: The three main types of machine learning are:\n",
      "\n",
      "1. **Supervised Learning**: In supervised learning, the model is trained on labeled data, which means that each training example is paired with an output label. The algorithm learns to map inputs to the correct outputs by minimizing the error between predicted and actual values. Common applications include classification tasks (e.g., spam detection) and regression tasks (e.g., predicting house prices).\n",
      "\n",
      "2. **Unsupervised Learning**: Unsupervised learning involves training a model on data that does not have labeled outputs. The goal is to find hidden patterns or intrinsic structures within the data. Common techniques include clustering (e.g., grouping customers by purchasing behavior) and dimensionality reduction (e.g., simplifying data by reducing the number of features).\n",
      "\n",
      "3. **Reinforcement Learning**: In reinforcement learning, an agent learns to make decisions by taking actions in an environment to maximize a cumulative reward. The agent receives feedback from the environment based on its actions, allowing it to learn optimal strategies over time. This type of learning is commonly used in robotics, gaming, and navigation tasks.\n",
      "\n",
      "These three types of machine learning encompass a wide range of algorithms and applications, addressing different types of problems and data structures.\n",
      "\n",
      "Relevance Score: 0.77\n",
      "Specificity Score: 0.68\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'relevance': 0.767517, 'specificity': 0.6836734693877551}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def automated_evaluation(prompt, response, expected_content):\n",
    "    \"\"\"Perform automated evaluation of a prompt-response pair.\"\"\"\n",
    "    relevance = relevance_score(response, expected_content)\n",
    "    specificity = specificity_score(response)\n",
    "    \n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    print(f\"Response: {response}\")\n",
    "    print(f\"\\nRelevance Score: {relevance:.2f}\")\n",
    "    print(f\"Specificity Score: {specificity:.2f}\")\n",
    "    \n",
    "    return {\"relevance\": relevance, \"specificity\": specificity}\n",
    "\n",
    "# Example usage\n",
    "prompt = \"What are the three main types of machine learning?\"\n",
    "expected_content = \"The three main types of machine learning are supervised learning, unsupervised learning, and reinforcement learning.\"\n",
    "response = llm.invoke(prompt).content\n",
    "automated_evaluation(prompt, response, expected_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ec72ab31-dbac-428e-acbd-d7ae9ebe7976",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Revathi\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1784: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: List the types of machine learning.\n",
      "Response: Machine learning can generally be categorized into several types based on how learning is structured and the type of feedback provided. The primary types include:\n",
      "\n",
      "1. **Supervised Learning**: In this approach, the model is trained on a labeled dataset, meaning that each training example is paired with an output label. The goal is to learn a mapping from inputs to outputs. Common algorithms include:\n",
      "   - Linear Regression\n",
      "   - Logistic Regression\n",
      "   - Decision Trees\n",
      "   - Support Vector Machines (SVM)\n",
      "   - Neural Networks\n",
      "   - Random Forests\n",
      "\n",
      "2. **Unsupervised Learning**: This type involves training on data without labeled responses. The goal is to identify patterns or structures within the data. Common techniques include:\n",
      "   - Clustering (e.g., K-Means, Hierarchical Clustering)\n",
      "   - Dimensionality Reduction (e.g., Principal Component Analysis, t-SNE)\n",
      "   - Association Rule Learning (e.g., Apriori Algorithm)\n",
      "\n",
      "3. **Semi-Supervised Learning**: This approach uses a combination of a small amount of labeled data and a large amount of unlabeled data for training. It leverages the strengths of both supervised and unsupervised learning.\n",
      "\n",
      "4. **Reinforcement Learning**: In this type, an agent learns by interacting with an environment, receiving feedback in the form of rewards or penalties. The goal is to maximize cumulative reward through trial and error. Key concepts include:\n",
      "   - Markov Decision Processes (MDP)\n",
      "   - Q-Learning\n",
      "   - Deep Q-Networks (DQN)\n",
      "   - Policy Gradient Methods\n",
      "\n",
      "5. **Self-Supervised Learning**: This is a subset of unsupervised learning where the model generates supervisory signals from the data itself. It is commonly seen in natural language processing and computer vision tasks.\n",
      "\n",
      "6. **Transfer Learning**: This involves taking a pre-trained model on one task and fine-tuning it for a different but related task. It is especially common in deep learning applications.\n",
      "\n",
      "7. **Multi-Task Learning**: This type involves training a model on multiple tasks simultaneously, allowing it to leverage commonalities and differences across tasks.\n",
      "\n",
      "Each of these types has its own applications, strengths, and weaknesses, and the choice of which to use depends on the specific problem at hand.\n",
      "\n",
      "Relevance Score: 0.73\n",
      "Specificity Score: 0.63\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Revathi\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1784: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: What are the main categories of machine learning algorithms?\n",
      "Response: Machine learning algorithms can be broadly categorized into several main categories based on how they learn from data and the type of tasks they are designed to perform. Here are the primary categories:\n",
      "\n",
      "1. **Supervised Learning**:\n",
      "   - Algorithms are trained on labeled data, meaning that the input data is paired with the correct output (the label).\n",
      "   - Common tasks include classification (e.g., identifying categories) and regression (e.g., predicting continuous values).\n",
      "   - Examples: Linear Regression, Logistic Regression, Decision Trees, Support Vector Machines (SVM), Neural Networks.\n",
      "\n",
      "2. **Unsupervised Learning**:\n",
      "   - Algorithms are trained on unlabeled data, where the system tries to learn the underlying structure of the data without explicit output labels.\n",
      "   - Common tasks include clustering (grouping similar instances) and dimensionality reduction (reducing the number of features).\n",
      "   - Examples: K-Means Clustering, Hierarchical Clustering, Principal Component Analysis (PCA), t-Distributed Stochastic Neighbor Embedding (t-SNE).\n",
      "\n",
      "3. **Semi-Supervised Learning**:\n",
      "   - Combines both labeled and unlabeled data, where a small amount of labeled data is used to improve learning on a larger set of unlabeled data.\n",
      "   - It is particularly useful when labeling data is expensive or time-consuming.\n",
      "   - Examples: Semi-Supervised Support Vector Machines, Ladder Networks.\n",
      "\n",
      "4. **Reinforcement Learning**:\n",
      "   - A type of learning where an agent learns to make decisions by taking actions in an environment to maximize a cumulative reward.\n",
      "   - The learning is driven by feedback received as rewards or penalties based on the actions taken.\n",
      "   - Examples: Q-Learning, Deep Q-Networks (DQN), Proximal Policy Optimization (PPO).\n",
      "\n",
      "5. **Self-Supervised Learning**:\n",
      "   - A subset of unsupervised learning where the system generates its own labels from the input data to learn useful representations.\n",
      "   - Often used in tasks such as natural language processing and computer vision.\n",
      "   - Examples: Contrastive Learning, Masked Language Models, Image Inpainting.\n",
      "\n",
      "6. **Multi-Instance Learning**:\n",
      "   - A form of supervised learning where the algorithm receives labeled bags (sets) of instances, rather than individual labeled instances.\n",
      "   - Commonly used in scenarios where individual data points are difficult to label.\n",
      "\n",
      "7. **Transfer Learning**:\n",
      "   - Involves taking a pre-trained model on one task and fine-tuning it on a different but related task, leveraging the learned features from the original task.\n",
      "   - Useful when there is limited labeled data for the target task.\n",
      "\n",
      "These categories cover a wide range of algorithms and approaches in machine learning, each suited to different types of problems and data characteristics.\n",
      "\n",
      "Relevance Score: 0.68\n",
      "Specificity Score: 0.60\n",
      "Prompt: Explain the different approaches to machine learning.\n",
      "Response: Machine learning (ML) encompasses a variety of approaches and methods, which can generally be categorized into several key types. The most prominent categories include:\n",
      "\n",
      "### 1. Supervised Learning\n",
      "In supervised learning, the model is trained on a labeled dataset, meaning that each training example is paired with an output label or target value. The goal is to learn a mapping from inputs to outputs so that, given new inputs, the model can predict the corresponding outputs.\n",
      "\n",
      "- **Common Algorithms**: Linear regression, logistic regression, support vector machines, decision trees, random forests, neural networks.\n",
      "- **Use Cases**: Classification (e.g., spam detection, image classification) and regression (e.g., predicting house prices).\n",
      "\n",
      "### 2. Unsupervised Learning\n",
      "Unsupervised learning involves training a model on data without explicit labels. The goal here is to find patterns or underlying structures in the data.\n",
      "\n",
      "- **Common Algorithms**: K-means clustering, hierarchical clustering, DBSCAN, principal component analysis (PCA), t-distributed stochastic neighbor embedding (t-SNE).\n",
      "- **Use Cases**: Clustering (e.g., customer segmentation), dimensionality reduction, anomaly detection, and feature extraction.\n",
      "\n",
      "### 3. Semi-Supervised Learning\n",
      "Semi-supervised learning is a middle ground between supervised and unsupervised learning. It utilizes a small amount of labeled data mixed with a large amount of unlabeled data. This approach is particularly useful when labeling data is expensive or time-consuming.\n",
      "\n",
      "- **Common Techniques**: Self-training, co-training, and graph-based methods.\n",
      "- **Use Cases**: Scenarios like image recognition where obtaining labels is expensive, but unlabeled data is plentiful.\n",
      "\n",
      "### 4. Reinforcement Learning\n",
      "Reinforcement learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, RL doesn't rely on labeled input-output pairs but rather on the agent's own experience.\n",
      "\n",
      "- **Key Concepts**: States, actions, rewards, policy, and value functions.\n",
      "- **Common Algorithms**: Q-learning, deep Q-networks (DQN), policy gradients, and actor-critic methods.\n",
      "- **Use Cases**: Game playing (e.g., AlphaGo), robotics, self-driving cars, recommendation systems, and various automated decision-making tasks.\n",
      "\n",
      "### 5. Transfer Learning\n",
      "Transfer learning involves taking a pre-trained model (usually from a large dataset or a related task) and fine-tuning it for a specific task. This is often useful when the target task has limited data.\n",
      "\n",
      "- **Application**: Common in natural language processing (NLP) and computer vision, where models like BERT or convolutional neural networks (CNNs) are adapted for specific applications.\n",
      "  \n",
      "### 6. Ensemble Learning\n",
      "Ensemble learning combines multiple models to improve performance. By leveraging the diverse strengths of different models, ensembles often yield better predictions than individual models.\n",
      "\n",
      "- **Common Techniques**: Bagging (e.g., Random Forests), boosting (e.g., AdaBoost, Gradient Boosting Machines), and stacking.\n",
      "- **Advantages**: Reduces overfitting and improves accuracy.\n",
      "\n",
      "### 7. Deep Learning\n",
      "Deep learning is a subset of machine learning that employs neural networks with many layers (deep networks). It is particularly suited for large datasets and complex tasks, enabling the automatic extraction of features.\n",
      "\n",
      "- **Common Architectures**: Convolutional neural networks (CNNs) for image data, recurrent neural networks (RNNs) for sequential data, and transformers for NLP tasks.\n",
      "- **Use Cases**: Image recognition, natural language processing, speech recognition, and game AI.\n",
      "\n",
      "### Conclusion\n",
      "Each of these approaches has its own strengths and is suitable for different types of problems. The choice of approach often depends on the nature of the data, the specific task at hand, and the computational resources available. Understanding these different approaches enables practitioners to select the most appropriate method for their machine learning projects.\n",
      "\n",
      "Relevance Score: 0.67\n",
      "Specificity Score: 0.60\n",
      "Prompt Comparison Results:\n",
      "\n",
      "1. Prompt: List the types of machine learning.\n",
      "   Relevance: 0.73\n",
      "   Specificity: 0.63\n",
      "\n",
      "2. Prompt: What are the main categories of machine learning algorithms?\n",
      "   Relevance: 0.68\n",
      "   Specificity: 0.60\n",
      "\n",
      "3. Prompt: Explain the different approaches to machine learning.\n",
      "   Relevance: 0.67\n",
      "   Specificity: 0.60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Revathi\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1784: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'prompt': 'List the types of machine learning.',\n",
       "  'relevance': 0.73224485,\n",
       "  'specificity': 0.6276923076923077},\n",
       " {'prompt': 'What are the main categories of machine learning algorithms?',\n",
       "  'relevance': 0.67915523,\n",
       "  'specificity': 0.5989717223650386},\n",
       " {'prompt': 'Explain the different approaches to machine learning.',\n",
       "  'relevance': 0.66815615,\n",
       "  'specificity': 0.5982300884955752}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compare_prompts(prompts, expected_content):\n",
    "    \"\"\"Compare the effectiveness of multiple prompts for the same task.\"\"\"\n",
    "    results = []\n",
    "    for prompt in prompts:\n",
    "        response = llm.invoke(prompt).content\n",
    "        evaluation = automated_evaluation(prompt, response, expected_content)\n",
    "        results.append({\"prompt\": prompt, **evaluation})\n",
    "    \n",
    "    # Sort results by relevance score\n",
    "    sorted_results = sorted(results, key=lambda x: x['relevance'], reverse=True)\n",
    "    \n",
    "    print(\"Prompt Comparison Results:\")\n",
    "    for i, result in enumerate(sorted_results, 1):\n",
    "        print(f\"\\n{i}. Prompt: {result['prompt']}\")\n",
    "        print(f\"   Relevance: {result['relevance']:.2f}\")\n",
    "        print(f\"   Specificity: {result['specificity']:.2f}\")\n",
    "    \n",
    "    return sorted_results\n",
    "\n",
    "# Example usage\n",
    "prompts = [\n",
    "    \"List the types of machine learning.\",\n",
    "    \"What are the main categories of machine learning algorithms?\",\n",
    "    \"Explain the different approaches to machine learning.\"\n",
    "]\n",
    "expected_content = \"The main types of machine learning are supervised learning, unsupervised learning, and reinforcement learning.\"\n",
    "compare_prompts(prompts, expected_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc472ca8-9a71-41a1-a850-1a2ec76eb86f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Revathi\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1784: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automated Evaluation:\n",
      "Prompt: Explain the concept of overfitting in machine learning.\n",
      "Response: Overfitting in machine learning refers to a modeling error that occurs when a machine learning model captures noise or random fluctuations in the training data, rather than the underlying patterns that generalize to unseen data. When a model is overfitted, it performs well on the training dataset but poorly on validation or test datasets because it has learned to recognize too many specific details and anomalies in the training data, rather than the underlying relationships.\n",
      "\n",
      "### Key Characteristics of Overfitting:\n",
      "\n",
      "1. **High Training Accuracy and Low Testing Accuracy**: An overfitted model shows very high performance (accuracy, error rate, etc.) on the training set while exhibiting significantly lower performance on unseen data.\n",
      "\n",
      "2. **Complexity of the Model**: Overfitting is often associated with models that are too complex relative to the amount of training data available. Simple models may underfit, while overly complex models can learn the noise in the data.\n",
      "\n",
      "3. **Lack of Generalizability**: Because overfitted models are tailored to the specific training data, they lack the ability to generalize to other datasets or new examples.\n",
      "\n",
      "### Causes of Overfitting:\n",
      "\n",
      "- **Complex Models**: Using highly complex models (like deep neural networks with many layers) on small datasets can lead to overfitting.\n",
      "- **Insufficient Training Data**: A small dataset may not adequately capture the underlying trends and patterns, causing the model to learn from noise instead.\n",
      "- **Noise in the Data**: If there is a significant amount of noise or outliers in the training data, the model might learn these rather than the actual signal.\n",
      "\n",
      "### How to Identify Overfitting:\n",
      "\n",
      "- **Learning Curves**: Plotting the training and validation loss against epochs can help visualize overfitting. If the training loss continues to decrease while the validation loss increases, the model is likely overfitting.\n",
      "- **Performance Metrics**: Significant disparities in performance metrics between the training and validation sets can indicate overfitting.\n",
      "\n",
      "### Strategies to Prevent Overfitting:\n",
      "\n",
      "1. **Simplifying the Model**: Using a less complex model can help capture the essential patterns without falling into noise.\n",
      "  \n",
      "2. **Regularization**: Techniques like L1 (lasso) and L2 (ridge) regularization add penalties for large coefficients, helping to reduce overfitting.\n",
      "\n",
      "3. **Cross-Validation**: Using multiple validation splits can help ensure the model's performance is robust and not merely a result of overfitting to a particular validation set.\n",
      "\n",
      "4. **Pruning**: In decision trees, pruning branches that have little importance can help to avoid capturing noise.\n",
      "\n",
      "5. **Early Stopping**: Monitoring the model’s performance on a validation set during training and stopping when performance starts to degrade can prevent overfitting.\n",
      "\n",
      "6. **Data Augmentation**: Increasing the amount of training data through augmentation techniques can help the model generalize better.\n",
      "\n",
      "7. **Ensemble Methods**: Combining predictions from multiple models can reduce the risk of overfitting when using complex models.\n",
      "\n",
      "In summary, overfitting is a critical concept in machine learning that reflects the balance between a model’s ability to learn from data and its capacity to generalize to new, unseen data. Recognizing and addressing overfitting is essential for building robust and effective machine learning systems.\n",
      "\n",
      "Relevance Score: 0.78\n",
      "Specificity Score: 0.50\n",
      "\n",
      "Manual Evaluation:\n",
      "Prompt: Explain the concept of overfitting in machine learning.\n",
      "Response: Overfitting in machine learning refers to a modeling error that occurs when a machine learning model captures noise or random fluctuations in the training data, rather than the underlying patterns that generalize to unseen data. When a model is overfitted, it performs well on the training dataset but poorly on validation or test datasets because it has learned to recognize too many specific details and anomalies in the training data, rather than the underlying relationships.\n",
      "\n",
      "### Key Characteristics of Overfitting:\n",
      "\n",
      "1. **High Training Accuracy and Low Testing Accuracy**: An overfitted model shows very high performance (accuracy, error rate, etc.) on the training set while exhibiting significantly lower performance on unseen data.\n",
      "\n",
      "2. **Complexity of the Model**: Overfitting is often associated with models that are too complex relative to the amount of training data available. Simple models may underfit, while overly complex models can learn the noise in the data.\n",
      "\n",
      "3. **Lack of Generalizability**: Because overfitted models are tailored to the specific training data, they lack the ability to generalize to other datasets or new examples.\n",
      "\n",
      "### Causes of Overfitting:\n",
      "\n",
      "- **Complex Models**: Using highly complex models (like deep neural networks with many layers) on small datasets can lead to overfitting.\n",
      "- **Insufficient Training Data**: A small dataset may not adequately capture the underlying trends and patterns, causing the model to learn from noise instead.\n",
      "- **Noise in the Data**: If there is a significant amount of noise or outliers in the training data, the model might learn these rather than the actual signal.\n",
      "\n",
      "### How to Identify Overfitting:\n",
      "\n",
      "- **Learning Curves**: Plotting the training and validation loss against epochs can help visualize overfitting. If the training loss continues to decrease while the validation loss increases, the model is likely overfitting.\n",
      "- **Performance Metrics**: Significant disparities in performance metrics between the training and validation sets can indicate overfitting.\n",
      "\n",
      "### Strategies to Prevent Overfitting:\n",
      "\n",
      "1. **Simplifying the Model**: Using a less complex model can help capture the essential patterns without falling into noise.\n",
      "  \n",
      "2. **Regularization**: Techniques like L1 (lasso) and L2 (ridge) regularization add penalties for large coefficients, helping to reduce overfitting.\n",
      "\n",
      "3. **Cross-Validation**: Using multiple validation splits can help ensure the model's performance is robust and not merely a result of overfitting to a particular validation set.\n",
      "\n",
      "4. **Pruning**: In decision trees, pruning branches that have little importance can help to avoid capturing noise.\n",
      "\n",
      "5. **Early Stopping**: Monitoring the model’s performance on a validation set during training and stopping when performance starts to degrade can prevent overfitting.\n",
      "\n",
      "6. **Data Augmentation**: Increasing the amount of training data through augmentation techniques can help the model generalize better.\n",
      "\n",
      "7. **Ensemble Methods**: Combining predictions from multiple models can reduce the risk of overfitting when using complex models.\n",
      "\n",
      "In summary, overfitting is a critical concept in machine learning that reflects the balance between a model’s ability to learn from data and its capacity to generalize to new, unseen data. Recognizing and addressing overfitting is essential for building robust and effective machine learning systems.\n",
      "\n",
      "Evaluation Criteria:\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Score for Clarity (0-10):  8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clarity: 8.0/10\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Score for Accuracy (0-10):  10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 10.0/10\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Score for Relevance (0-10):  10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relevance: 10.0/10\n",
      "\n",
      "Additional Comments:\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter any additional comments:  best prompt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comments: best prompt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'prompt': 'Explain the concept of overfitting in machine learning.',\n",
       " 'response': \"Overfitting in machine learning refers to a modeling error that occurs when a machine learning model captures noise or random fluctuations in the training data, rather than the underlying patterns that generalize to unseen data. When a model is overfitted, it performs well on the training dataset but poorly on validation or test datasets because it has learned to recognize too many specific details and anomalies in the training data, rather than the underlying relationships.\\n\\n### Key Characteristics of Overfitting:\\n\\n1. **High Training Accuracy and Low Testing Accuracy**: An overfitted model shows very high performance (accuracy, error rate, etc.) on the training set while exhibiting significantly lower performance on unseen data.\\n\\n2. **Complexity of the Model**: Overfitting is often associated with models that are too complex relative to the amount of training data available. Simple models may underfit, while overly complex models can learn the noise in the data.\\n\\n3. **Lack of Generalizability**: Because overfitted models are tailored to the specific training data, they lack the ability to generalize to other datasets or new examples.\\n\\n### Causes of Overfitting:\\n\\n- **Complex Models**: Using highly complex models (like deep neural networks with many layers) on small datasets can lead to overfitting.\\n- **Insufficient Training Data**: A small dataset may not adequately capture the underlying trends and patterns, causing the model to learn from noise instead.\\n- **Noise in the Data**: If there is a significant amount of noise or outliers in the training data, the model might learn these rather than the actual signal.\\n\\n### How to Identify Overfitting:\\n\\n- **Learning Curves**: Plotting the training and validation loss against epochs can help visualize overfitting. If the training loss continues to decrease while the validation loss increases, the model is likely overfitting.\\n- **Performance Metrics**: Significant disparities in performance metrics between the training and validation sets can indicate overfitting.\\n\\n### Strategies to Prevent Overfitting:\\n\\n1. **Simplifying the Model**: Using a less complex model can help capture the essential patterns without falling into noise.\\n  \\n2. **Regularization**: Techniques like L1 (lasso) and L2 (ridge) regularization add penalties for large coefficients, helping to reduce overfitting.\\n\\n3. **Cross-Validation**: Using multiple validation splits can help ensure the model's performance is robust and not merely a result of overfitting to a particular validation set.\\n\\n4. **Pruning**: In decision trees, pruning branches that have little importance can help to avoid capturing noise.\\n\\n5. **Early Stopping**: Monitoring the model’s performance on a validation set during training and stopping when performance starts to degrade can prevent overfitting.\\n\\n6. **Data Augmentation**: Increasing the amount of training data through augmentation techniques can help the model generalize better.\\n\\n7. **Ensemble Methods**: Combining predictions from multiple models can reduce the risk of overfitting when using complex models.\\n\\nIn summary, overfitting is a critical concept in machine learning that reflects the balance between a model’s ability to learn from data and its capacity to generalize to new, unseen data. Recognizing and addressing overfitting is essential for building robust and effective machine learning systems.\",\n",
       " 'relevance': 0.7781843,\n",
       " 'specificity': 0.5030181086519114}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def evaluate_prompt(prompt, expected_content, manual_criteria=['Clarity', 'Accuracy', 'Relevance']):\n",
    "    \"\"\"Perform a comprehensive evaluation of a prompt using both manual and automated techniques.\"\"\"\n",
    "    response = llm.invoke(prompt).content\n",
    "    \n",
    "    print(\"Automated Evaluation:\")\n",
    "    auto_results = automated_evaluation(prompt, response, expected_content)\n",
    "    \n",
    "    print(\"\\nManual Evaluation:\")\n",
    "    manual_evaluation(prompt, response, manual_criteria)\n",
    "    \n",
    "    return {\"prompt\": prompt, \"response\": response, **auto_results}\n",
    "\n",
    "# Example usage\n",
    "prompt = \"Explain the concept of overfitting in machine learning.\"\n",
    "expected_content = \"Overfitting occurs when a model learns the training data too well, including its noise and fluctuations, leading to poor generalization on new, unseen data.\"\n",
    "evaluate_prompt(prompt, expected_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e1c60a-c993-4d91-812d-49762677df2a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
